# -*- coding: utf-8 -*-
"""SHAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sravNLU460JwMXPy3HjXuxWZpT5_Vrhb
"""

!pip install aix360

from __future__ import print_function
import sklearn
from sklearn.model_selection import train_test_split
import sklearn.datasets
import sklearn.ensemble
import numpy as np
import time
np.random.seed(1)

# Importing shap KernelExplainer (aix360 style)
from aix360.algorithms.shap import KernelExplainer

# the following import is required for access to shap plotting functions and datasets
import shap

"""# Load Data"""

X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)

# rather than use the whole training set to estimate expected values, we could summarize with
# a set of weighted kmeans, each weighted by the number of points they represent. But this dataset
# is so small we don't worry about it
#X_train_summary = shap.kmeans(X_train, 50)

def print_accuracy(f):
    print("Accuracy = {0}%".format(100*np.sum(f(X_test) == Y_test)/len(Y_test)))
    time.sleep(0.5) # to let the print get out before any progress bars

shap.initjs()

# This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica)
X_train

Y_train

"""# KNN"""

knn = sklearn.neighbors.KNeighborsClassifier()
knn.fit(X_train, Y_train)

print_accuracy(knn.predict)



"""# PDP & ICE"""

from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.inspection import PartialDependenceDisplay

# ICE and PDP that show dependence between the target function (probability that a given instance belongs to class 2 - Virginica) and an input feature of interest
PartialDependenceDisplay.from_estimator(knn, X_train, [0], target=2, kind='both')
PartialDependenceDisplay.from_estimator(knn, X_train, [1], target=2, kind='both')
PartialDependenceDisplay.from_estimator(knn, X_train, [2], target=2, kind='both')
PartialDependenceDisplay.from_estimator(knn, X_train, [3], target=2, kind='both')



"""# SHAP"""

shapexplainer = KernelExplainer(knn.predict_proba, X_train)
print(type(shapexplainer))

# average prediction of the model of each class
avg_prediction = shapexplainer.explainer.expected_value
avg_prediction

X_test.iloc[0,:]

# aix360 style for explaining input instances
shap_values = shapexplainer.explain_instance(X_test.iloc[0,:]) # first instance in test

#shap_values of each feature in each class
shap_values

print('actual prediction (probability of the instance is in class 2):', knn.predict_proba(np.array([X_test.iloc[0]]))[0,2])
print('average prediction (probability of the instance is in class 2:', avg_prediction[2])
print('difference:', knn.predict_proba(np.array([X_test.iloc[0]]))[0,2] - avg_prediction[2])
print('the difference is explained by:')
shap.plots.bar.bar_plot(shap_values[2])

print('the sum of the shap vlaue is equal to the difference')
shap_values[2].sum()



"""# LIME"""

# Importing LimeTextExplainer (aix360 sytle)
from aix360.algorithms.lime import LimeTabularExplainer

limeexplainer = LimeTabularExplainer(X_train.to_numpy(), feature_names = X_train.columns, class_names = [0,1,2], mode = 'classification')
print(type(limeexplainer))

idx = 0
# aix360 style for explaining input instances
exp = limeexplainer.explain_instance(X_test.iloc[0,:], knn.predict_proba, labels = [0,1,2])
print('Predicted class =', knn.predict(np.array([X_test.iloc[0]]))[0])
print('True class: %s' % Y_test[0])

exp.as_pyplot_figure(label=0)
exp.as_pyplot_figure(label=1)
exp.as_pyplot_figure(label=2)
from matplotlib import pyplot as plt
plt.tight_layout()

X_test.iloc[0]





