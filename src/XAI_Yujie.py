# -*- coding: utf-8 -*-
"""DSC180A WEEK9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PlJ4Nf3jOfG4cCbAtK7gVtmEQb2poBFz
"""

!pip install aix360
# Importing LimeTabularExplainer (aix360 sytle)
from aix360.algorithms.lime import LimeTabularExplainer

!pip install anchor
import anchor

import pandas as pd
from __future__ import print_function
import sklearn
import numpy as np
import sklearn
import sklearn.ensemble
import sklearn.metrics
from sklearn.impute import SimpleImputer
from statistics import mean
from sklearn.ensemble import GradientBoostingClassifier

from google.colab import drive 
drive.mount('/content/gdrive')
train=pd.read_csv('gdrive/My Drive/train.csv')
test = pd.read_csv('gdrive/My Drive/test.csv')

train_y = train.Survived

"""**Model**"""

clf = GradientBoostingClassifier()
titanic_X_colns = ['PassengerId','Age', 'Fare',]
titanic_X = train[titanic_X_colns]
test_X = test[titanic_X_colns]
my_imputer = SimpleImputer()
imputed_titanic_X = my_imputer.fit_transform(titanic_X)
imputed_test_X = my_imputer.fit_transform(test_X)

clf.fit(imputed_titanic_X, train_y)

pred_y = clf.predict_proba(imputed_test_X)

"""**Partial Dependence Plot**"""

from sklearn.inspection import PartialDependenceDisplay

titanic_plots = PartialDependenceDisplay.from_estimator(clf, features=[1,2], X=imputed_titanic_X, 
                                        feature_names=titanic_X_colns)

"""**LIME**

When dealing with image classification, LIME does not perturb individual pixels as randomly changing the individual pixels won't change the prediction by much. Instead, it segments the image into superpixels based on similar colors and turning superpixels on or off.
"""

limeexplainer = LimeTabularExplainer(imputed_titanic_X, 
                                     feature_names=titanic_X_colns, 
                                     mode='classification')

# Now explain a prediction
exp = limeexplainer.explain_instance(imputed_test_X[0], 
                                     clf.predict_proba,
                                     labels = [0,1])

print('Predicted: ', pred_y[0])

exp.as_pyplot_figure(label = 0)
from matplotlib import pyplot as plt
plt.tight_layout()

exp.as_pyplot_figure(label = 1)
plt.tight_layout()

"""**SHAP**

https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d

https://github.com/Trusted-AI/AIX360/blob/master/examples/shap/SHAP.ipynb
"""

!pip install shap
import shap

from aix360.algorithms.shap import KernelExplainer

shapexplainer = KernelExplainer(clf.predict_proba, imputed_titanic_X)

# aix360 style for explaining input instances
shap_values = shapexplainer.explain_instance(imputed_test_X[0])

shapexplainer.explainer.expected_value

shap.initjs()
#Red/blue: Features that push the prediction higher (to the right) are shown in red, 
#and those pushing the prediction lower are in blue.
shap.force_plot(shapexplainer.explainer.expected_value[0], shap_values[0], imputed_test_X[0])

# feature values
imputed_test_X[0]

#output value
pred_y[0] #The passenger doesn't survive

"""**Anchors**

https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation_anchor.ipynb
"""

# I'm having trouble when importing anchor_tabular and utils
from anchor import utils
from anchor import anchor_tabular

anchorexplainer = anchor_text.AnchorText(clf, [0,1], use_unk_distribution=True)
exp = anchorexplainer.explain_instance(imputed_test_X[0], estimator, threshold=0.8, use_proba=True, batch_size=30)



